# Music Information Retrieval


### Project Discription

In this project I train a neural network to perform automatic transcription.  Using the Keras deep learning library on AWS EC2 gpu instances, I train 
neural networks using the GuitarSet dataset for guitar & the Maestro piano dataset for transcrription of piano pieces.  End goal would
be to allow a user to upload an audio recording, and to return a transscription.  I am currently having difficulty processing the output
of the neural network.  It appears as though I need to use a hidden markov model in order to map probabilities of notes of a certain frequency at a 
certain time.  


### Guitar Tab Transcription

I attempt to reproduce the neural network archticture proposed by Manuel Minguez Carretero in his thesis.  
For convenience I have added the PDF of this work to the repository.  He proposes several neural network architectures 
for solving this problem, which he trained on the MusicNet database, an MIR dataset of piano recordings and sheet music.  
Instead I am training & testing my network with data from, GuitarSet, a different MIR dataset of guitar recordings and 
and note labels. 

<img src="https://dakobed-style.s3-us-west-2.amazonaws.com/audio.png" height="120">
<br>
Raw Audio Signal 
 
 The task of generating guitar tablature is more difficult than generating piano sheet music (there are 
multiple ways to play each note on the guitar, whereas there is a one to one correspondence between notes and keys and the
piano).  Similarly to Carretero, I use the Librosa library to perform the transforms (transforming the raw audio signal into the 
frequency domain).  


<img src="https://dakobed-style.s3-us-west-2.amazonaws.com/screenshot.png" width="740" height="350">

Spectogram generated by Librosa

I save the the transforms and processed annotation numpy arrays to S3 from where the are downloaded to the EC2 instance, and fed into a Keras generator   
for training the neural network.  The annotation labels of MusicNet are GuitarSet are similar. For each note they provide the following features
* note start time
* note end time
* note (These are represented by MIDI values for each dataset though the MusicNet's have been rounded to integers while 
the GuitarSet's have not.  However owing to the fact that piano sheet music encapsulates more information (most notably
 note durations such as quarter notes, half notes whole notes etc) than does guitar tablature, the MusicNet labels has 
 additional features (note length, measure and beat) than does the GuitarSet. My earlier attempts ignored this feature, 
 however eventually I decided to attempt to populate the missing feature by solving an unsupervised learning problem.  


<img src="https://dakobed-style.s3-us-west-2.amazonaws.com/cnn.png" width="560" height="320">

Neural network proposed by Carretero


### Piano Music Transcription

I perform the same analysis using the Maestro dataset of piano pieces & midi files.  This dataset is significantly bigger.  


### Reproduction of the work ### 
- Download the data
    - process_guitarset_transfroms.py
    - https://magenta.tensorflow.org/datasets/maestro
- preprocess the data
    - python3 process_guitarset_transfroms.py

- Launch the EC2 GPU deep learning AMI instance 
    bash ec2_mir/launch.sh   
- Once model has completed training, make predictions w/ the model    
    - python3 make_predictions_with_model.py

